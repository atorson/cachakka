grpc {
  port = 50020
  services = ${?grpc.services}["com.cachakka.streaming.akka.grpc.GrpcServerHealthcheckService",
    "com.cachakka.streaming.akka.grpc.GrpcShardingRegionManagementService"]
}

akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  stdout-loglevel = "INFO"
  log-config-on-start = "off"

  # Core Actor settings
  actor {
    //allow-java-serialization = off
    enable-additional-serialization-bindings = on
    provider = "akka.cluster.ClusterActorRefProvider"
    serializers {
      json = "com.cachakka.streaming.akka.AkkaJsonSerializer"
      proto = "akka.remote.serialization.ProtobufSerializer"
      scala_proto = "com.cachakka.streaming.akka.AkkaProtoSerializer"
    }
    serialization-bindings {
      "com.trueaccord.scalapb.GeneratedMessage" = scala_proto
    }
    serialization-identifiers {
      "com.cachakka.streaming.akka.AkkaJsonSerializer" = 91
      "com.cachakka.streaming.akka.AkkaProtoSerializer" = 92
    }

    default-dispatcher{
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 16

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 3.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 64

        # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
        # like peeking mode which "pop".
        task-peeking-mode = "FIFO"
      }
    }
  }



  # Core Cluster settings
  cluster {

    # how long to wait for one of the seed nodes to reply to initial join request
    seed-node-timeout = 10s

    # If a join request fails it will be retried after this period.
    # Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s

    # Should the 'leader' in the cluster be allowed to automatically mark
    # unreachable nodes as DOWN after a configured time of unreachability?
    # Using auto-down implies that two separate clusters will automatically be
    # formed in case of network partition.
    #
    # Don't enable this in production, see 'Auto-downing (DO NOT USE)' section
    # of Akka Cluster documentation.
    #
    # Disable with "off" or specify a duration to enable auto-down.
    # If a downing-provider-class is configured this setting is ignored.
    auto-down-unreachable-after = off

    # Time margin after which shards or singletons that belonged to a downed/removed
    # partition are created in surviving partition. The purpose of this margin is that
    # in case of a network partition the persistent actors in the non-surviving partitions
    # must be stopped before corresponding persistent actors are started somewhere else.
    # This is useful if you implement downing strategies that handle network partitions,
    # e.g. by keeping the larger side of the partition and shutting down the smaller side.
    # It will not add any extra safety for auto-down-unreachable-after, since that is not
    # handling network partitions.
    # Disable with "off" or specify a duration to enable.
    down-removal-margin = 120s

    # All downing decisions will only be made if there were no new cluster membership events during that interval
    stable-after = 180s

    # Custom split brain resolver
    downing-provider-class = "com.cachakka.streaming.akka.RoleBasedSplitBrainResolverProvider"


    # The roles of this member. List of strings, e.g. roles = ["A", "B"].
    # The roles are part of the membership information and can be used by
    # routers or other services to distribute work to certain member types,
    # e.g. front-end and back-end nodes.
    roles = ["cacheworker"]

    # Enable/disable info level logging of cluster events
    log-info = on

    # Enable or disable JMX MBeans for management of the cluster
    jmx.enabled = on

    # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf
    # [Hayashibara et al]) used by the cluster subsystem to detect unreachable
    # members.
    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within
    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,
    # i.e. around 5.5 seconds with default settings.
    failure-detector {

      # Defines the failure detector threshold.
      # A low threshold is prone to generate many wrong suspicions but ensures
      # a quick detection in the event of a real crash. Conversely, a high
      # threshold generates fewer mistakes but needs more time to detect
      # actual crashes.
      threshold = 12.0

      # Number of member nodes that each member will send heartbeat messages to,
      # i.e. each node will be monitored by this number of other nodes.
      monitored-by-nr-of-members = 5

      # Number of potentially lost/delayed heartbeats that will be
      # accepted before considering it to be an anomaly.
      # This margin is important to be able to survive sudden, occasional,
      # pauses in heartbeat arrivals, due to for example garbage collect or
      # network drop.
      acceptable-heartbeat-pause = 120 s

      # After the heartbeat request has been sent the first failure detection
      # will start after this period, even though no heartbeat message has
      # been received.
      expected-response-after = 10 s

    }

  }

  # Terminate app when actor system shuts down
  coordinated-shutdown.exit-jvm = on

  # Settings for the ClusterShardingExtension
  cluster.sharding {

    # When this is set to 'on' the active entity actors will automatically be restarted
    # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
    # due to rebalance or crash.
    remember-entities = off

    # Defines how the coordinator stores its state. Same is also used by the
    # shards for rememberEntities.
    # Valid values are "ddata" or "persistence".
    state-store-mode = "persistence"

  }


}

dispatchers{

  misc {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "thread-pool-executor"

    thread-pool-executor {
      fixed-pool-size = 32
      task-queue-size = -1
      task-queue-type = "linked"
    }

    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 8
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 16
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }

  bulkheading {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 32
      task-queue-size = -1
      task-queue-type = "linked"
    }
    throughput = 1
  }

  affinity {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the thread pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 8
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 1
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 16
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }

  sharding{
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 16
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 64
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }

}