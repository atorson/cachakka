grpc {
  port = 50020
  services = ${?grpc.services}["com.cachakka.streaming.akka.grpc.GrpcServerHealthcheckService",
  "com.cachakka.streaming.akka.grpc.GrpcShardingRegionManagementService"]
}

akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  stdout-loglevel = "INFO"
  log-config-on-start = "off"

    # Core Actor settings
    actor {
      //allow-java-serialization = off
      enable-additional-serialization-bindings = on
      provider = "akka.cluster.ClusterActorRefProvider"
      serializers {
        json = "com.cachakka.streaming.akka.AkkaJsonSerializer"
        proto = "akka.remote.serialization.ProtobufSerializer"
        scala_proto = "com.cachakka.streaming.akka.AkkaProtoSerializer"
      }
      serialization-bindings {
        "com.trueaccord.scalapb.GeneratedMessage" = scala_proto
      }
      serialization-identifiers {
        "com.cachakka.streaming.akka.AkkaJsonSerializer" = 91
        "com.cachakka.streaming.akka.AkkaProtoSerializer" = 92
      }

      default-dispatcher{
        fork-join-executor {
          # Min number of threads to cap factor-based parallelism number to
          parallelism-min = 8

          # The parallelism factor is used to determine thread pool size using the
          # following formula: ceil(available processors * factor). Resulting size
          # is then bounded by the parallelism-min and parallelism-max values.
          parallelism-factor = 2.0

          # Max number of threads to cap factor-based parallelism number to
          parallelism-max = 32

          # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
          # like peeking mode which "pop".
          task-peeking-mode = "FIFO"
        }
      }
    }

    remote {

      netty.tcp {
        port = 2550
        maximum-frame-size = 12800000b
        send-buffer-size = 25600000b
        receive-buffer-size = 25600000b
        hostname = "localhost"
        bind-hostname = "0.0.0.0"
      }

      log-remote-lifecycle-events = off

    }

    # Core Cluster settings
    cluster {

      # seed node URLs list (comma-delimited)
      seed-nodes = ["akka.tcp://flink-colo-akka@localhost:2550"]

      # how long to wait for one of the seed nodes to reply to initial join request
      seed-node-timeout = 10s

      # If a join request fails it will be retried after this period.
      # Disable join retry by specifying "off".
      retry-unsuccessful-join-after = 10s

      # Should the 'leader' in the cluster be allowed to automatically mark
      # unreachable nodes as DOWN after a configured time of unreachability?
      # Using auto-down implies that two separate clusters will automatically be
      # formed in case of network partition.
      #
      # Don't enable this in production, see 'Auto-downing (DO NOT USE)' section
      # of Akka Cluster documentation.
      #
      # Disable with "off" or specify a duration to enable auto-down.
      # If a downing-provider-class is configured this setting is ignored.
      auto-down-unreachable-after = off

      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # It will not add any extra safety for auto-down-unreachable-after, since that is not
      # handling network partitions.
      # Disable with "off" or specify a duration to enable.
      down-removal-margin = 10s

      # All downing decisions will only be made if there were no new cluster membership events during that interval
      stable-after = 120s

      # Custom split brain resolver
      downing-provider-class = "com.cachakka.streaming.akka.RoleBasedSplitBrainResolverProvider"


      # The roles of this member. List of strings, e.g. roles = ["A", "B"].
      # The roles are part of the membership information and can be used by
      # routers or other services to distribute work to certain member types,
      # e.g. front-end and back-end nodes.
      roles = ["cachemanager", "cacheworker"]

      role {
        # Minimum required number of members of a certain role before the leader
        # changes member status of 'Joining' members to 'Up'. Typically used together
        # with 'Cluster.registerOnMemberUp' to defer some action, such as starting
        # actors, until the cluster has reached a certain size.
        # E.g. to require 2 nodes with role 'frontend' and 3 nodes with role 'backend':
        #   frontend.min-nr-of-members = 2
        #   backend.min-nr-of-members = 3
        #<role-name>.min-nr-of-members = 1
        cachemanager.min-nr-of-members = 1
      }

      # Minimum required number of members before the leader changes member status
      # of 'Joining' members to 'Up'. Typically used together with
      # 'Cluster.registerOnMemberUp' to defer some action, such as starting actors,
      # until the cluster has reached a certain size.
      min-nr-of-members = 1

      # Enable/disable info level logging of cluster events
      log-info = on

      # Enable or disable JMX MBeans for management of the cluster
      jmx.enabled = on

      # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf
      # [Hayashibara et al]) used by the cluster subsystem to detect unreachable
      # members.
      # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within
      # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,
      # i.e. around 5.5 seconds with default settings.
      failure-detector {

        # Defines the failure detector threshold.
        # A low threshold is prone to generate many wrong suspicions but ensures
        # a quick detection in the event of a real crash. Conversely, a high
        # threshold generates fewer mistakes but needs more time to detect
        # actual crashes.
        threshold = 12.0

        # Number of member nodes that each member will send heartbeat messages to,
        # i.e. each node will be monitored by this number of other nodes.
        monitored-by-nr-of-members = 1

      }

    }

    # HTTP
    http{
      server{
        idle-timeout = 1800s
      }
      host-connection-pool{
        idle-timeout = 1800s
      }
    }

    # Terminate app when actor system shuts down
    coordinated-shutdown.exit-jvm = on

    # Settings for the ClusterShardingExtension
    cluster.sharding {

      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # Defines how the coordinator stores its state. Same is also used by the
      # shards for rememberEntities.
      # Valid values are "ddata" or "persistence".
      state-store-mode = "persistence"

    }

    extensions = ${?akka.extensions}["akka.management.AkkaManagement"]

    management {
      http {
        # The hostname where the HTTP Server for Http Cluster Management will be started.
        # This defines the interface to use.
        # InetAddress.getLocalHost.getHostAddress is used not overriden or empty
        hostname = "localhost"

        # The port where the HTTP Server for Http Cluster Management will be bound.
        # The value will need to be from 0 to 65535.
        port = 1091

        # Use this setting to bind a network interface to a different hostname or ip
        # than the HTTP Server for Http Cluster Management.
        # Use "0.0.0.0" to bind to all interfaces.
        # akka.management.http.hostname if empty
        bind-hostname = "0.0.0.0"


        # path prefix for all management routes, usually best to keep the default value here. If
        # specified, you'll want to use the same value for all nodes that use akka management so
        # that they can know which path to access each other on.
        base-path = ""

        # List FQCN of management route providers which shall contribute routes to the management HTTP endpoint.
        # Management route providers should be are regular extensions that aditionally extend the
        # `akka.management.http.ManagementRoutesProvider` interface.
        #
        # Libraries may register routes into the management routes by appending entries to this setting:
        # `route-providers += "com.example.MyExtraHealthCheckRoutes"` in the library `reference.conf`.
        #
        # Should not be set by end user applications in 'application.conf', use the extensions property for that
        #
        route-providers = ["com.cachakka.streaming.akka.ClusterHttpManagement"]

      }

    }

}

dispatchers{

  misc {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    # This will be used if you have set "executor = "thread-pool-executor""
    # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor
    executor = "thread-pool-executor"

    thread-pool-executor {
      fixed-pool-size = 16
      task-queue-size = -1
      task-queue-type = "linked"
    }

    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 4
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 16
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }

  bulkheading {
    type = Dispatcher
    executor = "thread-pool-executor"

    thread-pool-executor {
      fixed-pool-size = 16
      task-queue-size = -1
      task-queue-type = "linked"
    }
    throughput = 1
  }

  affinity {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the thread pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 4
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 1
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 8
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }

  sharding{
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 8
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 32
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }

}
